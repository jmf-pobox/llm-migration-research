\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Cross-Language Code Migration Using Multi-Agent Systems:\\Evaluating Small-Scale Agentic Program Translation}

\author{James Freeman\\
\textit{Pembroke College, University of Oxford}\\
\texttt{james.freeman@pmb.ox.ac.uk}}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This paper demonstrates that LLM-based multi-agent systems can reliably perform automated cross-language code migration for small-scale, well-structured codebases. We present empirical validation using a 990-line Python codebase migrated to two structurally different target languages: Rust (a systems language with ownership semantics) and Java (a managed language with garbage collection). Both migrations achieved 100\% behavioral equivalence with the source implementation, as verified through input-output contract validation. The Rust migration produced 1,158 lines with 97.66\% test coverage at \$3.74 USD; the Java migration produced 1,262 lines with 95.87\% test coverage at \$7.24 USD. We evaluated two migration strategies: \textit{module-by-module} (vertical slices) and \textit{feature-by-feature} (horizontal slices), finding both produce identical behavioral output with different code organization. Our four-phase methodology---I/O contract generation, source analysis, sequential migration, and behavioral review---proved language-agnostic, requiring only configuration changes to target different languages. These results establish that automated migration is achievable for codebases exhibiting low cyclomatic complexity, strong module boundaries, and comprehensive test coverage. The methodology's limitations at this scale---primarily the trivial nature of the subject system---motivate future research into scaling these techniques to medium-complexity codebases with cross-module dependencies and external integrations.
\end{abstract}

\section{Introduction}

Cross-language code migration represents a significant challenge in software engineering. Organizations frequently need to port codebases between languages for performance optimization, ecosystem alignment, or maintainability improvements. Traditional approaches require substantial manual effort, with developers reading source code, understanding its semantics, and rewriting equivalent implementations in the target language.

Recent advances in Large Language Models have enabled automated code generation at unprecedented quality levels. However, applying these capabilities to complete codebase migration introduces challenges beyond single-function generation. A successful migration must preserve not only the public API but also the precise behavioral semantics of the original implementation, including edge cases and error handling.

This paper presents evidence that LLM-based multi-agent systems can reliably automate cross-language migration for a specific class of codebases: small-scale systems with low complexity, well-defined module boundaries, and strong test coverage. Through six experimental runs---five migrating Python to Rust, and one migrating Python to Java---we developed and validated a four-phase methodology that achieves 100\% behavioral equivalence across multiple target languages. The successful migration to two structurally different languages (Rust's ownership model versus Java's garbage collection) demonstrates that our methodology is language-agnostic rather than target-specific.

\subsection{Problem Statement}

Traditional LLM-based code generation tools suffer from fundamental limitations when applied to migration tasks. First, most tools lack file system access, requiring developers to manually copy source code into prompts. This approach does not scale to multi-file codebases and prevents agents from exploring dependencies. Second, without build tool integration, generated code cannot be verified for correctness until manual compilation. Third, single-shot generation provides no mechanism for iterative refinement based on compilation errors or test failures.

Perhaps most critically, existing approaches conflate API compatibility with behavioral equivalence. A migration that preserves function signatures may still produce different outputs for identical inputs, particularly in edge cases involving operator precedence, associativity, or error handling.

\subsection{Research Questions}

This study addresses three primary research questions:

\begin{enumerate}
\item \textbf{Feasibility}: Can multi-agent LLM systems automate complete cross-language migration with verified behavioral equivalence?
\item \textbf{Generality}: Does a migration methodology developed for one target language transfer to structurally different target languages?
\item \textbf{Scope}: What characteristics of source codebases determine whether automated migration is viable?
\end{enumerate}

\subsection{Contributions}

This paper makes four contributions:

\begin{enumerate}
\item \textbf{Proof of concept}: We demonstrate that automated migration achieving 100\% behavioral equivalence is achievable for small-scale, well-structured codebases.
\item \textbf{Multi-language validation}: We show the same four-phase methodology succeeds for both Rust and Java, establishing language-agnostic generality.
\item \textbf{Methodology specification}: We define a repeatable four-phase process (I/O contract, analysis, migration, review) with explicit quality gates.
\item \textbf{Scope characterization}: We identify the characteristics that made our subject system amenable to automated migration, informing future scaling research.
\end{enumerate}

\section{Background}

\subsection{Subject System}

Our experiments used \texttt{rpn2tex}, a command-line tool that converts Reverse Polish Notation mathematical expressions to LaTeX format. The Python implementation comprises 990 lines of code across seven modules: token definitions, abstract syntax tree nodes, error handling, lexical analysis, parsing, LaTeX generation, and command-line interface.

We deliberately selected a \textit{trivial} subject system to establish baseline feasibility before investigating more complex scenarios. The system exhibits characteristics that represent ideal conditions for automated migration:

\begin{itemize}
\item \textbf{Low cyclomatic complexity}: Each function implements straightforward control flow without deep nesting or complex branching.
\item \textbf{Clear module boundaries}: Dependencies flow unidirectionally from tokens through lexer, parser, and generator.
\item \textbf{No external dependencies}: The codebase uses only Python standard library features.
\item \textbf{Deterministic behavior}: Identical inputs always produce identical outputs.
\item \textbf{Explicit error handling}: Error conditions are well-defined with position tracking.
\item \textbf{Comprehensive testability}: Input-output behavior is easily captured and verified.
\end{itemize}

\subsubsection{Complexity Metrics}

To quantify the system's complexity, we measured cyclomatic complexity using \texttt{lizard}, a multi-language code complexity analyzer. Table~\ref{tab:complexity} presents consistent metrics across all three implementations.

\begin{table}[h]
\centering
\caption{Complexity metrics across implementations}
\label{tab:complexity}
\begin{tabular}{@{}lccc@{}}
\toprule
Metric & Python & Rust & Java \\
\midrule
Prod.\ LOC & 352 & 408 & 529 \\
Functions & 25 & 32 & 42 \\
Avg CC & 2.8 & 2.4 & 2.9 \\
Max CC & 10 & 7 & 15 \\
\bottomrule
\end{tabular}
\end{table}

All three implementations exhibit low cyclomatic complexity, with averages below 3.0. The maximum complexity in any single function is 15 (Java's \texttt{Parser.parse}), which remains in the ``moderate'' range. For reference, a typical ``medium complexity'' codebase has average CC of 5--10, and ``high complexity'' systems exceed 15. The subject system's metrics confirm its classification as ``trivial'' from a complexity standpoint.

These characteristics represent a best-case scenario for automated migration. The research question is not whether this particular system can be migrated---it clearly can---but whether the methodology that succeeds here generalizes to other target languages and, eventually, to more complex systems.

\subsection{Multi-Agent Architecture}

All experiments employed the Claude Agent SDK, which enables spawning specialized subagents with different tool access and model configurations. The architecture consists of an orchestrating agent that coordinates specialized workers.

The analyst agent uses a lightweight model with read-only file access for codebase analysis. The migrator agent uses a more capable model with full tool access including file writing, editing, and shell command execution. The reviewer agent uses a lightweight model to validate generated code against specifications.

This separation allows cost optimization by using expensive models only for code generation while using cheaper models for analysis and validation tasks.

\subsection{Quality Gates}

Each module migration required passing four quality gates before proceeding. The Rust compiler must accept the code without errors. The Clippy linter must report zero warnings when run in strict mode. The code formatter must not require any changes. All unit tests and documentation tests must pass.

These gates ensure that generated code meets production quality standards and that issues are caught and corrected during migration rather than discovered later.

\section{Method}

\subsection{Four-Phase Migration Process}

Our methodology employs a four-phase process designed to ensure behavioral equivalence:

\textbf{Phase 0: I/O Contract Generation.} Before migration begins, the source implementation is executed on a curated set of test inputs covering basic operations, operator precedence, associativity, edge cases, and error conditions. The exact outputs are captured as a behavioral contract that target implementations must satisfy.

\textbf{Phase 1: Source Analysis.} An analyst agent reads all source files and produces a comprehensive migration specification document. This document captures module structure, dependencies, public APIs, and implementation details in a format optimized for migration agents.

\textbf{Phase 2: Sequential Migration.} Migrator agents receive the specification document and I/O contract, generating target language implementations for each module. Each module must pass quality gates (compilation, linting, formatting, tests) before proceeding.

\textbf{Phase 3: Behavioral Review.} Reviewer agents validate that generated code satisfies the I/O contract by executing all test cases and comparing outputs to the captured contract.

\subsection{Key Design Decisions}

Two design decisions proved critical during methodology development:

\textbf{Focused agent contexts.} Early experiments with embedding source code directly in prompts performed poorly---four times slower and 38\% more expensive than focused approaches. Large contexts caused response latency (single responses taking 20+ minutes) and agents ignored embedded content, performing redundant file operations. The multi-phase approach keeps each agent's context minimal.

\textbf{Behavioral contracts over API compatibility.} Initial migrations that passed all quality gates still exhibited 19\% behavioral discrepancies in output formatting. For example, the input \texttt{5 3 - 2 -} produced \texttt{\$5 - 3 - 2\$} in Python but \texttt{\$( 5 - 3 ) - 2\$} in early Rust migrations---mathematically equivalent but semantically different. The I/O contract phase eliminated these discrepancies.

\subsection{Metrics}

We measured wall-clock duration, API cost in US dollars, production lines of code, test coverage (line and function/method), and I/O contract match rate (percentage of test cases producing identical output to the source implementation).

\section{Results}

\subsection{Migration Outcomes}

We performed migrations to two structurally different target languages: Rust (a systems language with ownership semantics and no garbage collection) and Java (a managed language with garbage collection). Both migrations achieved 100\% behavioral equivalence on the 21-case I/O contract. Table~\ref{tab:multilang} summarizes the results.

\begin{table}[h]
\centering
\caption{Multi-language migration comparison}
\label{tab:multilang}
\begin{tabular}{@{}lcc@{}}
\toprule
Metric & Rust & Java \\
\midrule
Duration & 25 min & 25 min \\
Cost & \$3.74 & \$7.24 \\
Prod.\ LOC & 1,158 & 1,262 \\
Test LOC & 1,346 & 2,580 \\
Tests & 93 & 226 \\
Line cov. & 97.7\% & 95.9\% \\
I/O match & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

The Java migration cost approximately 2x more than Rust despite similar duration. This difference is attributable to the larger volume of generated test code (2,580 lines versus 1,346 lines) and additional iterations for Checkstyle compliance.

Both languages achieved comparable code expansion (1.2--1.3x) and coverage metrics, demonstrating that the methodology produces consistent quality across structurally different target languages. The successful Java migration validates that our four-phase approach is not Rust-specific but genuinely language-agnostic.

\section{Discussion}

\subsection{Why Smaller Contexts Outperform}

The counterintuitive finding that embedding source code degraded performance can be explained by several factors. In transformer-based models, attention complexity scales quadratically with context length. Larger contexts increase both latency per token and total tokens processed. When subagents inherit parent context, this cost multiplies across invocations.

Furthermore, large contexts dilute the signal of specific instructions. When source code is embedded alongside instructions, the model must attend to substantially more content to locate relevant information. A focused specification document provides higher signal-to-noise ratio.

Finally, agents exhibit emergent behavior that may not follow explicit instructions. Despite being told to use embedded content, agents still performed file operations. This suggests that behavioral constraints should be enforced through tool access rather than prompt instructions.

\subsection{Importance of Behavioral Contracts}

Migrations that pass all quality gates may still exhibit behavioral discrepancies. In our experiments, early migrations without I/O contracts showed 19\% of test cases producing different output despite successful compilation, linting, and test passage. Auto-generated tests validate implementation self-consistency but cannot detect semantic drift from the source implementation.

Input-output contracts provide an oracle derived from the source implementation's actual behavior. By capturing exact outputs for representative inputs, contracts enable detection of behavioral differences that might otherwise go unnoticed. This is particularly important for edge cases involving operator precedence, associativity, and error handling where reasonable implementations may differ.

\subsection{Migration Strategy Comparison}

We evaluated two migration strategies on the same subject system: \textit{module-by-module} (vertical slices) and \textit{feature-by-feature} (horizontal slices).

\textbf{Module-by-module} migrates each source module completely before proceeding to the next. The order follows the dependency graph: tokens, AST, error handling, lexer, parser, generator, CLI. Each module is independently testable after migration.

\textbf{Feature-by-feature} migrates horizontal slices across all modules. Each feature (e.g., ``addition operator'') is migrated through lexer, parser, and generator before proceeding to the next feature. This strategy enables incremental I/O validation per feature.

Table~\ref{tab:strategy} compares the two strategies for Rust migration.

\begin{table}[h]
\centering
\caption{Migration strategy comparison (Rust)}
\label{tab:strategy}
\begin{tabular}{@{}lrr@{}}
\toprule
Metric & Mod-by-Mod & Feat-by-Feat \\
\midrule
Duration & 25 min & 43 min \\
Prod.\ LOC & 1,184 & 931 \\
Test LOC & 1,346 & 126 \\
Tests & 93 & 51 \\
Ext.\ deps & 2 & 0 \\
I/O match & 100\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

Both strategies produced byte-identical output for all test cases. The 21\% production code difference is attributable to coding style choices (module-by-module used external crates for CLI parsing and error handling), not the migration strategy itself. Feature-by-feature took 72\% longer (43 vs 25 minutes) due to incremental I/O validation after each feature slice.

For trivial codebases, both strategies are viable. Feature-by-feature is recommended for larger codebases where high-complexity functions benefit from incremental feature isolation and per-feature I/O validation.

\subsection{Cost Model}

Based on our experiments, we propose a cost model for LLM-based migration. For a codebase of similar complexity to our subject system, expect costs of \$3.50 to \$5.00 per thousand lines of source code. Duration scales approximately linearly with module count at 3 to 10 minutes per module. Build verification cycles average 8 to 10 per module as agents iteratively fix compilation errors and linter warnings.

These costs compare favorably to manual migration effort while providing guarantees through quality gates and behavioral contracts that manual migration typically lacks.

\subsection{Scope Characterization and Limitations}

The success of our methodology must be understood within its demonstrated scope. Our subject system represents a deliberately \textit{trivial} case: under 1,000 lines, no external dependencies, low cyclomatic complexity, unidirectional module dependencies, and deterministic input-output behavior. These characteristics define the boundary conditions under which we have validated the approach.

\textbf{What we have proven}: LLM-based multi-agent migration can achieve 100\% behavioral equivalence for small-scale, well-structured codebases across multiple target languages.

\textbf{What remains unproven}: Whether the methodology scales to medium or large codebases with:
\begin{itemize}
\item External library dependencies requiring equivalent library selection in the target language
\item Cross-module circular dependencies or complex initialization ordering
\item Non-deterministic behavior (threading, I/O, randomness)
\item Legacy code patterns lacking clear module boundaries
\item Implicit behavioral contracts not captured by simple I/O testing
\end{itemize}

The 21-case behavioral contract, while comprehensive for our subject system, represents only the most straightforward form of behavioral specification. Systems with stateful behavior, side effects, or complex error recovery may require substantially richer contracts.

\section{Related Work}

This work builds on research in LLM-based code generation, multi-agent systems, and automated software migration. Prior work on code generation has focused primarily on single-function or single-file generation from natural language descriptions. Our work extends this to complete multi-file codebase migration with behavioral validation.

Multi-agent LLM systems have been explored for complex reasoning tasks, but their application to software engineering tasks with tool use remains limited. Our agent architecture with specialized roles and model selection contributes to understanding of effective multi-agent designs.

Automated software migration has traditionally relied on rule-based transformation systems or statistical translation models. LLM-based approaches offer greater flexibility but require careful attention to behavioral equivalence that rule-based systems handle implicitly.

\section{Conclusion}

This paper provides empirical evidence that LLM-based multi-agent systems can reliably automate cross-language code migration for small-scale, well-structured codebases. Through successful migrations to both Rust and Java, we have validated a four-phase methodology that achieves 100\% behavioral equivalence with the source implementation.

\textbf{Primary finding}: The methodology is language-agnostic. The same four-phase process---I/O contract generation, source analysis, sequential migration, and behavioral review---succeeded for both Rust (a systems language with ownership semantics) and Java (a managed language with garbage collection). Only configuration changes were required: build commands, file extensions, and idiom-specific prompts. This validates that the approach is not target-specific.

\textbf{Secondary finding}: Behavioral contracts are essential. Without explicit I/O contract validation, migrations may produce functionally correct implementations that differ semantically from the source. Early experiments showed 19\% behavioral discrepancies despite passing all compilation, linting, and testing gates, demonstrating that traditional quality metrics are insufficient for faithful migration.

\textbf{Methodological finding}: Smaller, focused agent contexts outperform large comprehensive prompts. Multi-phase orchestration with focused specification documents achieved results four times faster and 25\% cheaper than embedding source code directly in prompts.

Our subject system, \texttt{rpn2tex}, is deliberately trivial: under 1,000 lines, no external dependencies, low complexity, and deterministic behavior. This triviality is a feature, not a limitation. By demonstrating success on an ideal case, we establish that the fundamental approach works. The research question now shifts from feasibility to scalability.

\section{Future Work}

The validated methodology provides a foundation for scaling research. Key questions for future investigation include:

\textbf{Medium-complexity codebases}: How does the methodology perform on systems in the 5,000--20,000 LOC range with external dependencies? Our next target is \texttt{txt2tex}, a 9,300 LOC Python codebase with average cyclomatic complexity of 6.7 and maximum complexity of 40. The feature-by-feature strategy is expected to handle this complexity better than module-by-module due to its incremental validation approach.

\textbf{Strategy selection}: When should feature-by-feature be preferred over module-by-module? Our preliminary finding suggests feature-by-feature is better for codebases with cross-cutting concerns and high-complexity functions, while module-by-module works well for codebases with clear module boundaries.

\textbf{Dependency management}: Can agents automatically select equivalent libraries in the target ecosystem, or does this require human guidance? How should dependency version constraints be handled?

\textbf{Stateful systems}: How can behavioral contracts capture systems with internal state, side effects, or non-deterministic behavior? What contract languages are expressive enough for complex behavioral specifications?

\textbf{Incremental migration}: For large codebases, can migration proceed incrementally with interoperability between migrated and unmigrated modules? What FFI or serialization strategies enable gradual transition?

\textbf{Cost scaling}: Our cost model (\$3.50--\$5.00 per KLOC) is derived from a single trivial system. How do costs scale with complexity? Are there complexity thresholds beyond which automated migration becomes cost-prohibitive?

The success demonstrated in this paper represents a necessary but not sufficient condition for practical automated migration. We have proven that the approach works at small scale with two different strategies; the challenge now is determining how far it can scale.

\section*{Acknowledgments}

This research was conducted using the Claude Agent SDK with Claude Opus 4.5 and Claude 3.5 Haiku models. The experimental framework and analysis were developed collaboratively between human and AI researchers.

\bibliographystyle{plain}

\begin{thebibliography}{9}

\bibitem{anthropic2024}
Anthropic.
\textit{Claude Agent SDK Documentation}.
2024.

\bibitem{rust2024}
The Rust Programming Language.
\textit{The Rust Reference}.
2024.

\end{thebibliography}

\appendix

\section{Input-Output Contract}

The behavioral contract comprised 21 test cases across eight categories: basic binary operations (4 tests), unsupported operators (3 tests), operator precedence (3 tests), associativity (2 tests), addition chains (1 test), mixed operations (4 tests), floating-point numbers (2 tests), and complex expressions (2 tests).

Representative examples include: input \texttt{5 3 +} producing output \texttt{\$5 + 3\$}; input \texttt{5 3 + 2 *} producing output \texttt{\$( 5 + 3 ) \textbackslash times 2\$}; and input \texttt{2 3 \^{}} producing an error for unsupported operator.

\onecolumn
\section{Migration Framework Architecture}

This appendix describes the multi-agent framework used to perform automated migrations. The framework is implemented using the Claude Agent SDK and consists of four specialized agents coordinated by an orchestrator.

\subsection{Agent Roles}

The framework employs four distinct agent types, each with specific responsibilities and tool access:

\textbf{1. I/O Contract Agent} (Phase 0)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Generate behavioral specification by executing source implementation
\item \textit{Model}: Claude 3.5 Haiku (fast, low-cost)
\item \textit{Tools}: \texttt{Bash}, \texttt{Read}
\item \textit{Output}: Structured I/O contract mapping inputs to expected outputs
\end{itemize}

\textbf{2. Analyst Agent} (Phase 1)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Read all source files and produce migration specification
\item \textit{Model}: Claude 3.5 Haiku
\item \textit{Tools}: \texttt{Read}, \texttt{Glob}, \texttt{Grep} (read-only access)
\item \textit{Output}: Comprehensive specification document including I/O contract
\end{itemize}

\textbf{3. Migrator Agent} (Phase 2)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Convert individual modules to target language
\item \textit{Model}: Claude 3.5 Sonnet (more capable, higher cost)
\item \textit{Tools}: \texttt{Read}, \texttt{Write}, \texttt{Edit}, \texttt{Bash}, \texttt{Glob}, \texttt{Grep} (full access)
\item \textit{Output}: Target language source files passing quality gates
\end{itemize}

\textbf{4. Reviewer Agent} (Phase 3)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Validate migrated code against specification and I/O contract
\item \textit{Model}: Claude 3.5 Haiku
\item \textit{Tools}: \texttt{Read}, \texttt{Glob}, \texttt{Grep}, \texttt{Bash}
\item \textit{Output}: Review report with pass/fail verdict
\end{itemize}

\pagebreak

\subsection{Orchestration Flow}

The main orchestrator coordinates agent invocations following a strict phase order:

\begin{lstlisting}[language={}]
Orchestrator
    |
    +--[Phase 0]---> I/O Contract Agent
    |                    |
    |               (executes source on test inputs)
    |                    |
    |               I/O Contract Document
    |                    |
    +--[Phase 1]---> Analyst Agent
    |                    |
    |               (reads all source files)
    |                    |
    |               Migration Specification
    |               (includes I/O contract)
    |                    |
    +--[Phase 2]---> Migrator Agent (module 1)
    |                    |
    |               <-- Quality Gate Loop -->
    |                    |
    +--[Phase 3]---> Reviewer Agent (module 1)
    |                    |
    +--[Phase 2]---> Migrator Agent (module 2)
    |                    ...
    |               (repeat for each module)
    |                    |
    +--[Complete]
\end{lstlisting}

\subsection{Feedback Loops}

The framework implements two levels of feedback loops to ensure correctness:

\textbf{Inner Loop: Quality Gates}

During Phase 2, each migrator agent operates in a feedback loop with the build system:

\begin{lstlisting}[language={}]
Migrator writes code
        |
        v
    [Build/Compile]---fail---> Migrator reads errors
        |                           |
       pass                    fixes code
        |                           |
        v                           v
    [Lint/Format]---fail---> Migrator reads warnings
        |                           |
       pass                    fixes code
        |                           |
        v                           |
    [Run Tests]---fail--------> Migrator reads failures
        |                           |
       pass                    fixes code
        |                           |
        v                           |
    [I/O Contract]---fail----> Migrator adjusts logic
        |
       pass
        |
        v
    Module Complete
\end{lstlisting}

For Rust migrations, quality gates are: \texttt{cargo check}, \texttt{cargo clippy -{}- -D warnings}, \texttt{cargo fmt}, and \texttt{cargo test}. For Java: \texttt{./gradlew compileJava}, \texttt{./gradlew checkstyleMain}, and \texttt{./gradlew test}.

\textbf{Outer Loop: Review and Revision}

After migration, the reviewer agent validates against the specification. If issues are found, the orchestrator can re-invoke the migrator with reviewer feedback:

\begin{lstlisting}[language={}]
Migrator completes module
        |
        v
    Reviewer validates
        |
    +---pass---> Next module
    |
   fail
    |
    v
Orchestrator passes feedback to Migrator
        |
        v
    Migrator revises
        |
        v
    (re-enter quality gate loop)
\end{lstlisting}

\subsection{Language-Agnostic Design}

The framework abstracts language-specific details into a \texttt{LanguageTarget} interface:

\begin{lstlisting}[language={}]
class LanguageTarget(ABC):
    name: str              # "rust", "java"
    file_extension: str    # ".rs", ".java"

    def get_quality_gates() -> list[str]
    def get_migrator_idioms() -> str
    def get_reviewer_checks() -> str
    def get_file_mapping(source_file) -> str
\end{lstlisting}

Adding a new target language requires implementing this interface with language-specific build commands, idiom requirements, and file naming conventions. The agent prompts, orchestration logic, and feedback loops remain unchanged.

\subsection{Cost Optimization}

The framework optimizes API costs through model selection:
\begin{itemize}[nosep]
\item Expensive models (Sonnet) only for code generation requiring creativity
\item Cheaper models (Haiku) for analysis and validation tasks
\item Specification documents reduce redundant source file reads
\item Sequential module migration limits context size per invocation
\end{itemize}

\end{document}
