\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Cross-Language Code Migration Using Multi-Agent Systems:\\Evaluating Small-Scale Agentic Program Translation}

\author{James Freeman\\
\textit{Pembroke College, University of Oxford}\\
\texttt{james.freeman@pmb.ox.ac.uk}}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This paper demonstrates that LLM-based multi-agent systems can reliably perform automated cross-language code migration for small-scale, well-structured codebases. We present empirical validation using a 990-line Python codebase migrated to three structurally different target languages: Rust (a systems language with ownership semantics), Java (a managed language with garbage collection), and Go (a systems language with composition-based design). All migrations achieved 100\% behavioral equivalence with the source implementation, as verified through input-output contract validation. The Rust migration produced 579 lines at \$6.53 USD; the Java migration produced 368 lines at \$4.92 USD; the Go migration produced 781 lines at \$6.85 USD. We evaluated two migration strategies: \textit{module-by-module} (vertical slices) and \textit{feature-by-feature} (horizontal slices), finding both produce identical behavioral output with different code organization. Our four-phase methodology---I/O contract generation, source analysis, sequential migration, and behavioral review---proved language-agnostic, requiring only configuration changes to target different languages. These results establish that automated migration is achievable for codebases exhibiting low cyclomatic complexity, strong module boundaries, and comprehensive test coverage. The methodology's limitations at this scale---primarily the trivial nature of the subject system---motivate future research into scaling these techniques to medium-complexity codebases with cross-module dependencies and external integrations.
\end{abstract}

\section{Introduction}

Cross-language code migration represents a significant challenge in software engineering. Organizations frequently need to port codebases between languages for performance optimization, ecosystem alignment, or maintainability improvements. Traditional approaches require substantial manual effort, with developers reading source code, understanding its semantics, and rewriting equivalent implementations in the target language.

Recent advances in Large Language Models have enabled automated code generation at unprecedented quality levels. However, applying these capabilities to complete codebase migration introduces challenges beyond single-function generation. A successful migration must preserve not only the public API but also the precise behavioral semantics of the original implementation, including edge cases and error handling.

This paper presents evidence that LLM-based multi-agent systems can reliably automate cross-language migration for a specific class of codebases: small-scale systems with low complexity, well-defined module boundaries, and strong test coverage. Through experimental runs migrating Python to Rust, Java, and Go, we developed and validated a four-phase methodology that achieves 100\% behavioral equivalence across multiple target languages. The successful migration to three structurally different languages---Rust's ownership model, Java's garbage collection, and Go's composition-based design---demonstrates that our methodology is language-agnostic rather than target-specific.

\subsection{Problem Statement}

Traditional LLM-based code generation tools suffer from fundamental limitations when applied to migration tasks. First, most tools lack file system access, requiring developers to manually copy source code into prompts. This approach does not scale to multi-file codebases and prevents agents from exploring dependencies. Second, without build tool integration, generated code cannot be verified for correctness until manual compilation. Third, single-shot generation provides no mechanism for iterative refinement based on compilation errors or test failures.

Perhaps most critically, existing approaches conflate API compatibility with behavioral equivalence. A migration that preserves function signatures may still produce different outputs for identical inputs, particularly in edge cases involving operator precedence, associativity, or error handling.

\subsection{Research Questions}

This study addresses three primary research questions:

\begin{enumerate}
\item \textbf{Feasibility}: Can multi-agent LLM systems automate complete cross-language migration with verified behavioral equivalence?
\item \textbf{Generality}: Does a migration methodology developed for one target language transfer to structurally different target languages?
\item \textbf{Scope}: What characteristics of source codebases determine whether automated migration is viable?
\end{enumerate}

\subsection{Contributions}

This paper makes four contributions:

\begin{enumerate}
\item \textbf{Proof of concept}: We demonstrate that automated migration achieving 100\% behavioral equivalence is achievable for small-scale, well-structured codebases.
\item \textbf{Multi-language validation}: We show the same four-phase methodology succeeds for Rust, Java, and Go, establishing language-agnostic generality.
\item \textbf{Methodology specification}: We define a repeatable four-phase process (I/O contract, analysis, migration, review) with explicit quality gates.
\item \textbf{Scope characterization}: We identify the characteristics that made our subject system amenable to automated migration, informing future scaling research.
\end{enumerate}

\section{Background}

\subsection{Subject System}

Our experiments used \texttt{rpn2tex}, a command-line tool that converts Reverse Polish Notation mathematical expressions to LaTeX format. The Python implementation comprises 990 lines of code across seven modules: token definitions, abstract syntax tree nodes, error handling, lexical analysis, parsing, LaTeX generation, and command-line interface.

We deliberately selected a \textit{trivial} subject system to establish baseline feasibility before investigating more complex scenarios. The system exhibits characteristics that represent ideal conditions for automated migration:

\begin{itemize}
\item \textbf{Low cyclomatic complexity}: Each function implements straightforward control flow without deep nesting or complex branching.
\item \textbf{Clear module boundaries}: Dependencies flow unidirectionally from tokens through lexer, parser, and generator.
\item \textbf{No external dependencies}: The codebase uses only Python standard library features.
\item \textbf{Deterministic behavior}: Identical inputs always produce identical outputs.
\item \textbf{Explicit error handling}: Error conditions are well-defined with position tracking.
\item \textbf{Comprehensive testability}: Input-output behavior is easily captured and verified.
\end{itemize}

\subsubsection{Code Metrics}

Table~\ref{tab:complexity} presents production code metrics across all implementations, measured using line counts excluding test code.

\begin{table}[h]
\centering
\caption{Production code metrics (excluding tests)}
\label{tab:complexity}
\begin{tabular}{@{}llrr@{}}
\toprule
Target & Strategy & LOC & Funcs \\
\midrule
Python & (source) & 352 & 25 \\
\midrule
Rust & Mod-by-mod & 579 & 38 \\
Rust & Feat-by-feat & 549 & 33 \\
\midrule
Java & Mod-by-mod & 368 & 31 \\
Java & Feat-by-feat & 389 & 32 \\
\midrule
Go & Mod-by-mod & 781 & 42 \\
Go & Feat-by-feat & 613 & 35 \\
\bottomrule
\end{tabular}
\end{table}

Both migration strategies produced similar amounts of production code for each target language. Rust migrations expanded from 352 to approximately 560 LOC (1.6x), Java migrations expanded to approximately 380 LOC (1.1x), and Go migrations expanded to approximately 700 LOC (2.0x). Go's higher expansion ratio reflects explicit error handling with \texttt{if err != nil} patterns, verbose struct definitions, and the module-by-module strategy's tendency to generate more comprehensive test infrastructure.

These characteristics represent a best-case scenario for automated migration. The research question is not whether this particular system can be migrated---it clearly can---but whether the methodology that succeeds here generalizes to other target languages and, eventually, to more complex systems.

\subsection{Multi-Agent Architecture}

All experiments employed the Claude Agent SDK, which enables spawning specialized subagents with different tool access and model configurations. The architecture consists of an orchestrating agent that coordinates specialized workers.

The analyst agent uses a lightweight model with read-only file access for codebase analysis. The migrator agent uses a more capable model with full tool access including file writing, editing, and shell command execution. The reviewer agent uses a lightweight model to validate generated code against specifications.

This separation allows cost optimization by using expensive models only for code generation while using cheaper models for analysis and validation tasks.

\subsection{Quality Gates}

Each module migration required passing four quality gates before proceeding. The Rust compiler must accept the code without errors. The Clippy linter must report zero warnings when run in strict mode. The code formatter must not require any changes. All unit tests and documentation tests must pass.

These gates ensure that generated code meets production quality standards and that issues are caught and corrected during migration rather than discovered later.

\section{Method}

\subsection{Four-Phase Migration Process}

Our methodology employs a four-phase process designed to ensure behavioral equivalence:

\textbf{Phase 0: I/O Contract Generation.} Before migration begins, the source implementation is executed on a curated set of test inputs covering basic operations, operator precedence, associativity, edge cases, and error conditions. The exact outputs are captured as a behavioral contract that target implementations must satisfy.

\textbf{Phase 1: Source Analysis.} An analyst agent reads all source files and produces a comprehensive migration specification document. This document captures module structure, dependencies, public APIs, and implementation details in a format optimized for migration agents.

\textbf{Phase 2: Sequential Migration.} Migrator agents receive the specification document and I/O contract, generating target language implementations for each module. Each module must pass quality gates (compilation, linting, formatting, tests) before proceeding.

\textbf{Phase 3: Behavioral Review.} Reviewer agents validate that generated code satisfies the I/O contract by executing all test cases and comparing outputs to the captured contract.

\subsection{Key Design Decisions}

Two design decisions proved critical during methodology development:

\textbf{Focused agent contexts.} Early experiments with embedding source code directly in prompts performed poorly---four times slower and 38\% more expensive than focused approaches. Large contexts caused response latency (single responses taking 20+ minutes) and agents ignored embedded content, performing redundant file operations. The multi-phase approach keeps each agent's context minimal.

\textbf{Behavioral contracts over API compatibility.} Initial migrations that passed all quality gates still exhibited 19\% behavioral discrepancies in output formatting. For example, the input \texttt{5 3 - 2 -} produced \texttt{\$5 - 3 - 2\$} in Python but \texttt{\$( 5 - 3 ) - 2\$} in early Rust migrations---mathematically equivalent but semantically different. The I/O contract phase eliminated these discrepancies.

\subsection{Metrics}

We measured wall-clock duration, API cost in US dollars, production lines of code, test coverage (line and function/method), and I/O contract match rate (percentage of test cases producing identical output to the source implementation).

\section{Results}

\subsection{Migration Outcomes}

We performed migrations to three structurally different target languages: Rust (a systems language with ownership semantics), Java (a managed language with garbage collection), and Go (a systems language with composition-based design and explicit error handling). We tested both module-by-module and feature-by-feature strategies for each target. All migrations achieved 100\% behavioral equivalence on the 21-case I/O contract. Table~\ref{tab:multilang} summarizes the results.

\begin{table}[h]
\centering
\caption{Migration results by target and strategy}
\label{tab:multilang}
\begin{tabular}{@{}llrrr@{}}
\toprule
Target & Strategy & Dur. & Cost & Msgs \\
\midrule
Rust & Mod-by-mod & 32m & \$6.53 & 937 \\
Rust & Feat-by-feat & 32m & \$4.63 & 742 \\
Java & Mod-by-mod & 26m & \$4.92 & 871 \\
Java & Feat-by-feat & 51m & \$6.27 & 1026 \\
Go & Mod-by-mod & 29m & \$6.85 & 918 \\
Go & Feat-by-feat & 32m & \$5.60 & 581 \\
\bottomrule
\end{tabular}
\end{table}

All six migrations achieved 100\% I/O contract match. The observed cost and duration differences between strategies varied by target language: for Rust and Go, feature-by-feature was less expensive; for Java, module-by-module was less expensive.

\textbf{Limitations of single-run observations}: Each configuration was executed once. The migration process involves non-deterministic model behavior, and observed differences may reflect run-to-run variance rather than systematic strategy effects. Establishing statistically significant conclusions about strategy efficiency would require 5--10 runs per configuration to characterize the distribution of outcomes.

\section{Discussion}

\subsection{Why Smaller Contexts Outperform}

The counterintuitive finding that embedding source code degraded performance can be explained by several factors. In transformer-based models, attention complexity scales quadratically with context length. Larger contexts increase both latency per token and total tokens processed. When subagents inherit parent context, this cost multiplies across invocations.

Furthermore, large contexts dilute the signal of specific instructions. When source code is embedded alongside instructions, the model must attend to substantially more content to locate relevant information. A focused specification document provides higher signal-to-noise ratio.

Finally, agents exhibit emergent behavior that may not follow explicit instructions. Despite being told to use embedded content, agents still performed file operations. This suggests that behavioral constraints should be enforced through tool access rather than prompt instructions.

\subsection{Importance of Behavioral Contracts}

Migrations that pass all quality gates may still exhibit behavioral discrepancies. In our experiments, early migrations without I/O contracts showed 19\% of test cases producing different output despite successful compilation, linting, and test passage. Auto-generated tests validate implementation self-consistency but cannot detect semantic drift from the source implementation.

Input-output contracts provide an oracle derived from the source implementation's actual behavior. By capturing exact outputs for representative inputs, contracts enable detection of behavioral differences that might otherwise go unnoticed. This is particularly important for edge cases involving operator precedence, associativity, and error handling where reasonable implementations may differ.

\subsection{Migration Strategy Comparison}

We evaluated two migration strategies on the same subject system: \textit{module-by-module} (vertical slices) and \textit{feature-by-feature} (horizontal slices).

\textbf{Module-by-module} migrates each source module completely before proceeding to the next. The order follows the dependency graph: tokens, AST, error handling, lexer, parser, generator, CLI. Each module is independently testable after migration.

\textbf{Feature-by-feature} migrates horizontal slices across all modules. Each feature (e.g., ``addition operator'') is migrated through lexer, parser, and generator before proceeding to the next feature. This strategy enables incremental I/O validation per feature.

Table~\ref{tab:strategy} presents observed metrics for both strategies across target languages.

\begin{table}[h]
\centering
\caption{Strategy comparison by target language}
\label{tab:strategy}
\begin{tabular}{@{}llrrr@{}}
\toprule
Target & Strategy & Cost & API & Msgs \\
\midrule
Rust & Mod-by-mod & \$6.53 & 55m & 937 \\
Rust & Feat-by-feat & \$4.63 & 33m & 742 \\
Java & Mod-by-mod & \$4.92 & 45m & 871 \\
Java & Feat-by-feat & \$6.27 & 54m & 1026 \\
Go & Mod-by-mod & \$6.85 & 57m & 918 \\
Go & Feat-by-feat & \$5.60 & 34m & 581 \\
\bottomrule
\end{tabular}
\end{table}

Both strategies produced identical output for all test cases regardless of target language. The observed pattern---feature-by-feature appearing more efficient for Rust and Go while module-by-module appearing more efficient for Java---may reflect genuine differences in how each strategy interacts with target language characteristics, or may reflect run-to-run variance in non-deterministic model behavior.

We note several possible explanations for the observed differences, though none are confirmed:
\begin{itemize}
\item Rust's module system and cargo's incremental compilation may favor feature-by-feature's iterative approach
\item Go's flat package structure and fast compilation may similarly benefit from feature-by-feature's focused iterations
\item Java's class-based structure may align better with module-by-module's complete-class-at-a-time approach
\item The specific test cases and error recovery paths encountered in each run vary non-deterministically
\end{itemize}

Both strategies reliably produce correct output. For this trivial codebase, strategy selection does not affect correctness, only cost and duration.

\subsection{Tool Usage Patterns}

Table~\ref{tab:tools} shows tool invocation counts across all migrations. These counts reflect how agents interact with the codebase during migration.

\begin{table}[h]
\centering
\caption{Tool invocations by target and strategy}
\label{tab:tools}
\begin{tabular}{@{}llrrrr@{}}
\toprule
Target & Strategy & Bash & Read & Write & Edit \\
\midrule
Rust & Mod-by-mod & 301 & 134 & 18 & 46 \\
Rust & Feat-by-feat & 191 & 100 & 10 & 33 \\
Java & Mod-by-mod & 268 & 99 & 21 & 5 \\
Java & Feat-by-feat & 235 & 153 & 23 & 25 \\
Go & Mod-by-mod & 255 & 114 & 34 & 9 \\
Go & Feat-by-feat & 110 & 80 & 16 & 41 \\
\bottomrule
\end{tabular}
\end{table}

Several patterns emerge from the tool usage data. Module-by-module consistently uses more Bash invocations (average 275 vs 179), reflecting more frequent build/test cycles per module. Feature-by-feature shows higher Edit-to-Write ratios in some cases, suggesting more iterative refinement of existing files rather than complete rewrites. The Go feature-by-feature migration stands out with notably fewer Bash calls (110), indicating more efficient iteration.

\subsection{Test Coverage}

Table~\ref{tab:coverage} presents test coverage measurements for migrations where coverage tooling was available.

\begin{table}[h]
\centering
\caption{Test coverage by target and strategy}
\label{tab:coverage}
\begin{tabular}{@{}llrr@{}}
\toprule
Target & Strategy & Coverage & Tests \\
\midrule
Rust & Mod-by-mod & 96.9\% & 24 \\
Rust & Feat-by-feat & 84.9\% & 19 \\
Go & Mod-by-mod & 83.8\% & 230 \\
Go & Feat-by-feat & 74.1\% & 125 \\
Java & Mod-by-mod & --- & 0 \\
Java & Feat-by-feat & 72.0\% & 50 \\
\bottomrule
\end{tabular}
\end{table}

Rust migrations achieved the highest coverage: 96.9\% for module-by-module and 84.9\% for feature-by-feature, measured using cargo-llvm-cov. Go migrations achieved 83.8\% (module-by-module) and 74.1\% (feature-by-feature) using Go's built-in coverage tooling. Java feature-by-feature achieved 72.0\% coverage with 50 JUnit tests measured via JaCoCo. Java module-by-module produced standalone demonstration programs rather than JUnit tests, so coverage measurement was not possible.

These coverage gaps highlight a methodological limitation: the migration framework should ensure coverage tooling is configured during migration rather than requiring post-hoc measurement.

\subsection{Cost Observations}

Across all six migrations (three targets, two strategies each), observed costs ranged from \$4.63 to \$6.85, with durations from 26 to 51 minutes. For this 990-line source codebase, this corresponds to approximately \$4.70 to \$6.90 per thousand lines of source code.

These observations derive from single runs per configuration. The non-deterministic nature of LLM-based migration means individual runs may vary. To establish reliable cost estimates, multiple runs per configuration would be necessary to characterize the distribution.

We note that all runs completed successfully with 100\% I/O contract match, suggesting the methodology is reliable for correctness even if cost varies between runs.

\subsection{Scope Characterization and Limitations}

The success of our methodology must be understood within its demonstrated scope. Our subject system represents a deliberately \textit{trivial} case: under 1,000 lines, no external dependencies, low cyclomatic complexity, unidirectional module dependencies, and deterministic input-output behavior. These characteristics define the boundary conditions under which we have validated the approach.

\textbf{What we have proven}: LLM-based multi-agent migration can achieve 100\% behavioral equivalence for small-scale, well-structured codebases across multiple target languages.

\textbf{What remains unproven}: Whether the methodology scales to medium or large codebases with:
\begin{itemize}
\item External library dependencies requiring equivalent library selection in the target language
\item Cross-module circular dependencies or complex initialization ordering
\item Non-deterministic behavior (threading, I/O, randomness)
\item Legacy code patterns lacking clear module boundaries
\item Implicit behavioral contracts not captured by simple I/O testing
\end{itemize}

The 21-case behavioral contract, while comprehensive for our subject system, represents only the most straightforward form of behavioral specification. Systems with stateful behavior, side effects, or complex error recovery may require substantially richer contracts.

\section{Related Work}

This work builds on research in LLM-based code generation, multi-agent systems, and automated software migration. Prior work on code generation has focused primarily on single-function or single-file generation from natural language descriptions. Our work extends this to complete multi-file codebase migration with behavioral validation.

Multi-agent LLM systems have been explored for complex reasoning tasks, but their application to software engineering tasks with tool use remains limited. Our agent architecture with specialized roles and model selection contributes to understanding of effective multi-agent designs.

Automated software migration has traditionally relied on rule-based transformation systems or statistical translation models. LLM-based approaches offer greater flexibility but require careful attention to behavioral equivalence that rule-based systems handle implicitly.

\section{Conclusion}

This paper provides empirical evidence that LLM-based multi-agent systems can reliably automate cross-language code migration for small-scale, well-structured codebases. Through successful migrations to Rust, Java, and Go, we have validated a four-phase methodology that achieves 100\% behavioral equivalence with the source implementation.

\textbf{Primary finding}: The methodology is language-agnostic. The same four-phase process---I/O contract generation, source analysis, sequential migration, and behavioral review---succeeded for Rust (ownership semantics), Java (garbage collection), and Go (composition-based design). Only configuration changes were required: build commands, file extensions, and idiom-specific prompts. This validates that the approach is not target-specific.

\textbf{Secondary finding}: Behavioral contracts are essential. Without explicit I/O contract validation, migrations may produce functionally correct implementations that differ semantically from the source. Early experiments showed 19\% behavioral discrepancies despite passing all compilation, linting, and testing gates, demonstrating that traditional quality metrics are insufficient for faithful migration.

\textbf{Methodological finding}: Smaller, focused agent contexts outperform large comprehensive prompts. Multi-phase orchestration with focused specification documents achieved results four times faster and 25\% cheaper than embedding source code directly in prompts.

Our subject system, \texttt{rpn2tex}, is deliberately trivial: under 1,000 lines, no external dependencies, low complexity, and deterministic behavior. This triviality is a feature, not a limitation. By demonstrating success on an ideal case, we establish that the fundamental approach works. The research question now shifts from feasibility to scalability.

\section{Future Work}

The validated methodology provides a foundation for scaling research. Key questions for future investigation include:

\textbf{Medium-complexity codebases}: How does the methodology perform on systems in the 5,000--20,000 LOC range with external dependencies? Our next target is \texttt{txt2tex}, a 9,300 LOC Python codebase with average cyclomatic complexity of 6.7 and maximum complexity of 40. At this scale, the relative performance of feature-by-feature versus module-by-module strategies remains an open question.

\textbf{Strategy selection}: When should feature-by-feature be preferred over module-by-module? Our observations showed different patterns by target language, but with only one run per configuration, we cannot determine whether these reflect systematic differences or run-to-run variance. Multiple runs per configuration would be needed to identify reliable strategy selection criteria.

\textbf{Dependency management}: Can agents automatically select equivalent libraries in the target ecosystem, or does this require human guidance? How should dependency version constraints be handled?

\textbf{Stateful systems}: How can behavioral contracts capture systems with internal state, side effects, or non-deterministic behavior? What contract languages are expressive enough for complex behavioral specifications?

\textbf{Incremental migration}: For large codebases, can migration proceed incrementally with interoperability between migrated and unmigrated modules? What FFI or serialization strategies enable gradual transition?

\textbf{Cost scaling}: Observed costs ranged from \$4.70 to \$6.60 per KLOC across four migrations of a single trivial system. How do costs scale with complexity? Are there complexity thresholds beyond which automated migration becomes cost-prohibitive? Characterizing the cost distribution would require additional runs.

The success demonstrated in this paper represents a necessary but not sufficient condition for practical automated migration. We have proven that the approach works at small scale with two different strategies; the challenge now is determining how far it can scale.

\section*{Acknowledgments}

This research was conducted using the Claude Agent SDK with Claude Opus 4.5 and Claude 3.5 Haiku models. The experimental framework and analysis were developed collaboratively between human and AI researchers.

\bibliographystyle{plain}

\begin{thebibliography}{9}

\bibitem{anthropic2024}
Anthropic.
\textit{Claude Agent SDK Documentation}.
2024.

\bibitem{rust2024}
The Rust Programming Language.
\textit{The Rust Reference}.
2024.

\end{thebibliography}

\appendix

\section{Input-Output Contract}

The behavioral contract comprised 21 test cases across eight categories: basic binary operations (4 tests), unsupported operators (3 tests), operator precedence (3 tests), associativity (2 tests), addition chains (1 test), mixed operations (4 tests), floating-point numbers (2 tests), and complex expressions (2 tests).

Representative examples include: input \texttt{5 3 +} producing output \texttt{\$5 + 3\$}; input \texttt{5 3 + 2 *} producing output \texttt{\$( 5 + 3 ) \textbackslash times 2\$}; and input \texttt{2 3 \^{}} producing an error for unsupported operator.

\onecolumn
\section{Migration Framework Architecture}

This appendix describes the multi-agent framework used to perform automated migrations. The framework is implemented using the Claude Agent SDK and consists of four specialized agents coordinated by an orchestrator.

\subsection{Agent Roles}

The framework employs four distinct agent types, each with specific responsibilities and tool access:

\textbf{1. I/O Contract Agent} (Phase 0)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Generate behavioral specification by executing source implementation
\item \textit{Model}: Claude 3.5 Haiku (fast, low-cost)
\item \textit{Tools}: \texttt{Bash}, \texttt{Read}
\item \textit{Output}: Structured I/O contract mapping inputs to expected outputs
\end{itemize}

\textbf{2. Analyst Agent} (Phase 1)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Read all source files and produce migration specification
\item \textit{Model}: Claude 3.5 Haiku
\item \textit{Tools}: \texttt{Read}, \texttt{Glob}, \texttt{Grep} (read-only access)
\item \textit{Output}: Comprehensive specification document including I/O contract
\end{itemize}

\textbf{3. Migrator Agent} (Phase 2)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Convert individual modules to target language
\item \textit{Model}: Claude 3.5 Sonnet (more capable, higher cost)
\item \textit{Tools}: \texttt{Read}, \texttt{Write}, \texttt{Edit}, \texttt{Bash}, \texttt{Glob}, \texttt{Grep} (full access)
\item \textit{Output}: Target language source files passing quality gates
\end{itemize}

\textbf{4. Reviewer Agent} (Phase 3)
\begin{itemize}[itemsep=0pt]
\item \textit{Purpose}: Validate migrated code against specification and I/O contract
\item \textit{Model}: Claude 3.5 Haiku
\item \textit{Tools}: \texttt{Read}, \texttt{Glob}, \texttt{Grep}, \texttt{Bash}
\item \textit{Output}: Review report with pass/fail verdict
\end{itemize}

\pagebreak

\subsection{Orchestration Flow}

The main orchestrator coordinates agent invocations following a strict phase order:

\begin{lstlisting}[language={}]
Orchestrator
    |
    +--[Phase 0]---> I/O Contract Agent
    |                    |
    |               (executes source on test inputs)
    |                    |
    |               I/O Contract Document
    |                    |
    +--[Phase 1]---> Analyst Agent
    |                    |
    |               (reads all source files)
    |                    |
    |               Migration Specification
    |               (includes I/O contract)
    |                    |
    +--[Phase 2]---> Migrator Agent (module 1)
    |                    |
    |               <-- Quality Gate Loop -->
    |                    |
    +--[Phase 3]---> Reviewer Agent (module 1)
    |                    |
    +--[Phase 2]---> Migrator Agent (module 2)
    |                    ...
    |               (repeat for each module)
    |                    |
    +--[Complete]
\end{lstlisting}

\subsection{Feedback Loops}

The framework implements two levels of feedback loops to ensure correctness:

\textbf{Inner Loop: Quality Gates}

During Phase 2, each migrator agent operates in a feedback loop with the build system:

\begin{lstlisting}[language={}]
Migrator writes code
        |
        v
    [Build/Compile]---fail---> Migrator reads errors
        |                           |
       pass                    fixes code
        |                           |
        v                           v
    [Lint/Format]---fail---> Migrator reads warnings
        |                           |
       pass                    fixes code
        |                           |
        v                           |
    [Run Tests]---fail--------> Migrator reads failures
        |                           |
       pass                    fixes code
        |                           |
        v                           |
    [I/O Contract]---fail----> Migrator adjusts logic
        |
       pass
        |
        v
    Module Complete
\end{lstlisting}

For Rust migrations, quality gates are: \texttt{cargo check}, \texttt{cargo clippy -{}- -D warnings}, \texttt{cargo fmt}, and \texttt{cargo test}. For Java: \texttt{./gradlew compileJava}, \texttt{./gradlew checkstyleMain}, and \texttt{./gradlew test}. For Go: \texttt{go build}, \texttt{go vet}, \texttt{gofmt}, and \texttt{go test}.

\textbf{Outer Loop: Review and Revision}

After migration, the reviewer agent validates against the specification. If issues are found, the orchestrator can re-invoke the migrator with reviewer feedback:

\begin{lstlisting}[language={}]
Migrator completes module
        |
        v
    Reviewer validates
        |
    +---pass---> Next module
    |
   fail
    |
    v
Orchestrator passes feedback to Migrator
        |
        v
    Migrator revises
        |
        v
    (re-enter quality gate loop)
\end{lstlisting}

\subsection{Language-Agnostic Design}

The framework abstracts language-specific details into a \texttt{LanguageTarget} interface:

\begin{lstlisting}[language={}]
class LanguageTarget(ABC):
    name: str              # "rust", "java", "go"
    file_extension: str    # ".rs", ".java", ".go"

    def get_quality_gates() -> list[str]
    def get_migrator_idioms() -> str
    def get_reviewer_checks() -> str
    def get_file_mapping(source_file) -> str
\end{lstlisting}

Adding a new target language requires implementing this interface with language-specific build commands, idiom requirements, and file naming conventions. The agent prompts, orchestration logic, and feedback loops remain unchanged.

\subsection{Cost Optimization}

The framework optimizes API costs through model selection:
\begin{itemize}[nosep]
\item Expensive models (Sonnet) only for code generation requiring creativity
\item Cheaper models (Haiku) for analysis and validation tasks
\item Specification documents reduce redundant source file reads
\item Sequential module migration limits context size per invocation
\end{itemize}

\end{document}
