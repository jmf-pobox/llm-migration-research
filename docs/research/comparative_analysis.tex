\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\title{Autonomous Cross-Language Code Migration via Behavioral Contracts}

\author{James Freeman\\
\textit{Pembroke College, University of Oxford}\\
\texttt{james.freeman@pmb.ox.ac.uk}}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This paper evaluates LLM-based multi-agent systems for autonomous cross-language code migration using a 352-line Python codebase migrated to Rust, Java, and Go. Unlike AI-assisted approaches requiring human guidance, our migrations run end-to-end without human intervention: agents read source code, generate target implementations, execute quality gates, and iterate on failures autonomously. We performed 18 migrations---three runs each for two strategies (module-by-module and feature-by-feature) across three target languages---with automated metrics collection including cost, duration, lines of code, and test coverage. All migrations completed successfully with 100\% behavioral equivalence. Averaging across runs, costs ranged from \$6.32 to \$11.93 USD per migration; test coverage ranged from 67\% to 95\% depending on target language. Rust achieved the highest coverage (95\%), followed by Java (80--92\%) and Go (67--71\%). Module-by-module completed faster for Rust (32 min vs 51 min) but showed no consistent speed advantage across languages. Feature-by-feature was cheaper for Java and Go. The four-phase methodology---I/O contract generation, source analysis, sequential migration, and behavioral review---proved language-agnostic. These results establish a baseline demonstrating that automated migration is achievable for small-scale, well-structured codebases. Future research will determine whether this approach scales to medium-sized systems.
\end{abstract}

\section{Introduction}

Cross-language code migration represents a significant challenge in software engineering. Organizations maintaining legacy codebases---whether PHP monoliths, aging Java systems, or COBOL backends---frequently need to migrate to modern languages for performance optimization, developer availability, ecosystem alignment, or architectural modernization. Traditional approaches require substantial manual effort, with developers reading source code, understanding its semantics, and rewriting equivalent implementations in the target language. For large legacy systems, this effort can span years and consume significant engineering resources.

Recent advances in Large Language Models have enabled automated code generation at unprecedented quality levels~\cite{chen2021codex}. However, applying these capabilities to complete codebase migration introduces challenges beyond single-function generation. A successful migration must preserve not only the public API but also the precise behavioral semantics of the original implementation, including edge cases and error handling.

This paper presents evidence that LLM-based multi-agent systems can reliably automate cross-language migration for a specific class of codebases: small-scale systems with low complexity, well-defined module boundaries, and strong test coverage. Through 18 experimental runs---three per configuration---migrating Python to Rust, Java, and Go, we developed and validated a four-phase methodology that achieves 100\% behavioral equivalence across all runs and target languages. The successful migration to three structurally different languages---Rust's ownership model, Java's garbage collection, and Go's composition-based design---demonstrates that our methodology is language-agnostic rather than target-specific.

\subsection{Problem Statement}

Existing LLM-based migration approaches typically operate in an AI-assisted mode: developers prompt models for code suggestions, manually review and integrate outputs, and handle compilation errors themselves. This human-in-the-loop approach limits throughput and requires developer attention throughout the process.

Traditional LLM-based code generation tools suffer from fundamental limitations when applied to fully autonomous migration. First, most tools lack file system access, requiring developers to manually copy source code into prompts. This approach does not scale to multi-file codebases and prevents agents from exploring dependencies. Second, without build tool integration, generated code cannot be verified for correctness until manual compilation. Third, single-shot generation provides no mechanism for iterative refinement based on compilation errors or test failures.

Perhaps most critically, existing approaches conflate API compatibility with behavioral equivalence. A migration that preserves function signatures may still produce different outputs for identical inputs, particularly in edge cases involving operator precedence, associativity, or error handling.

\subsection{Research Questions}

This study addresses three primary research questions:

\begin{enumerate}
\item \textbf{Feasibility}: Can multi-agent LLM systems automate complete cross-language migration with verified behavioral equivalence?
\item \textbf{Generality}: Does a migration methodology developed for one target language transfer to structurally different target languages?
\item \textbf{Scope}: What characteristics of source codebases determine whether automated migration is viable?
\end{enumerate}

\subsection{Contributions}

This paper makes four contributions:

\begin{enumerate}
\item \textbf{Autonomous migration}: We demonstrate end-to-end migration without human intervention, where agents autonomously handle error recovery through feedback loops with compilers, linters, and test runners.
\item \textbf{Multi-language validation}: We show the same four-phase methodology succeeds for Rust, Java, and Go, establishing language-agnostic generality.
\item \textbf{Methodology specification}: We define a repeatable four-phase process (I/O contract, analysis, migration, review) with explicit quality gates.
\item \textbf{Scope characterization}: We identify the characteristics that made our subject system amenable to autonomous migration, informing future scaling research.
\end{enumerate}

\section{Background}

\subsection{Subject System}

Our experiments used \texttt{rpn2tex}, a command-line tool that converts Reverse Polish Notation mathematical expressions to LaTeX format. The Python implementation comprises 352 lines of production code across seven modules: token definitions, abstract syntax tree nodes, error handling, lexical analysis, parsing, LaTeX generation, and command-line interface.

We deliberately selected a \textit{trivial} subject system to establish baseline feasibility before investigating more complex scenarios. The system exhibits characteristics that represent ideal conditions for automated migration:

\begin{itemize}
\item \textbf{Low cyclomatic complexity}: Each function implements straightforward control flow without deep nesting or complex branching.
\item \textbf{Clear module boundaries}: Dependencies flow unidirectionally from tokens through lexer, parser, and generator.
\item \textbf{No external dependencies}: The codebase uses only Python standard library features.
\item \textbf{Deterministic behavior}: Identical inputs always produce identical outputs.
\item \textbf{Explicit error handling}: Error conditions are well-defined with position tracking.
\item \textbf{Comprehensive testability}: Input-output behavior is easily captured and verified.
\end{itemize}

\subsubsection{Code Metrics}

The source Python implementation contains 352 lines of production code and 204 lines of test code across 7 modules. Table~\ref{tab:loc} presents lines of code for all migrations, distinguishing production code from test code.

\begin{table}[h]
\centering
\caption{Lines of code by target and strategy (averaged across 3 runs)}
\label{tab:loc}
\begin{tabular}{@{}llrrr@{}}
\toprule
Target & Strategy & Prod & Test & Ratio \\
\midrule
Python & (source) & 352 & 204 & 0.6x \\
Rust & MbM & 617 & 1,665 & 2.7x \\
Rust & FbF & 462 & 1,362 & 2.9x \\
Java & MbM & 542 & 2,265 & 4.2x \\
Java & FbF & 404 & 948 & 2.3x \\
Go & MbM & 575 & 2,608 & 4.5x \\
Go & FbF & 405 & 1,320 & 3.3x \\
\bottomrule
\end{tabular}
\end{table}

A notable finding: while the source Python had a 0.6x test-to-production ratio (204 test LOC for 352 prod LOC), agents generated 2--5x more test code than production code in the target languages. The test-to-production ratio ranged from 2.3x (Java FbF) to 4.5x (Go MbM)---a 4--7x increase in relative test investment compared to the source. This suggests the quality gates and I/O contract requirements drive substantial test generation as agents work to satisfy behavioral validation criteria.

These characteristics represent a best-case scenario for automated migration. The research question is not whether this particular system can be migrated---it clearly can---but whether the methodology that succeeds here generalizes to other target languages and, eventually, to more complex systems.

\subsection{Multi-Agent Architecture}

All experiments employed the Claude Agent SDK~\cite{anthropic2024sdk}, which enables spawning specialized subagents with different tool access and model configurations. The architecture consists of an orchestrating agent that coordinates specialized workers.

The analyst agent uses a lightweight model with read-only file access for codebase analysis. The migrator agent uses a more capable model with full tool access including file writing, editing, and shell command execution. The reviewer agent uses a lightweight model to validate generated code against specifications.

This separation allows cost optimization by using expensive models only for code generation while using cheaper models for analysis and validation tasks.

\subsection{Quality Gates}

Each module migration required passing four quality gates before proceeding:

\begin{enumerate}
\item \textbf{Compilation}: The target language compiler must accept the code without errors (rustc, javac, or go build).
\item \textbf{Linting}: The language-specific linter must report zero warnings in strict mode (Clippy for Rust, Checkstyle for Java, go vet for Go).
\item \textbf{Formatting}: The code formatter must not require any changes (rustfmt, google-java-format, gofmt).
\item \textbf{Testing}: All unit tests must pass.
\end{enumerate}

These gates ensure that generated code meets production quality standards and that issues are caught and corrected during migration rather than discovered later. The autonomous feedback loop between code generation and quality gates allows agents to iterate on failures without human intervention.

\section{Method}

\subsection{Four-Phase Migration Process}

Our methodology employs a four-phase process designed to ensure behavioral equivalence:

\textbf{Phase 0: I/O Contract Generation.} Before migration begins, the source implementation is executed on a curated set of test inputs covering basic operations, operator precedence, associativity, edge cases, and error conditions. The exact outputs are captured as a behavioral contract that target implementations must satisfy.

\textbf{Phase 1: Source Analysis.} An analyst agent reads all source files and produces a comprehensive migration specification document. This document captures module structure, dependencies, public APIs, and implementation details in a format optimized for migration agents.

\textbf{Phase 2: Sequential Migration.} Migrator agents receive the specification document and I/O contract, generating target language implementations for each module. Each module must pass quality gates (compilation, linting, formatting, tests) before proceeding.

\textbf{Phase 3: Behavioral Review.} Reviewer agents validate that generated code satisfies the I/O contract by executing all test cases and comparing outputs to the captured contract.

\subsection{Key Design Decisions}

Two design decisions proved critical during methodology development:

\textbf{Focused agent contexts.} Early experiments with embedding source code directly in prompts performed poorly---four times slower and 38\% more expensive than focused approaches. Large contexts caused response latency (single responses taking 20+ minutes) and agents ignored embedded content, performing redundant file operations. The multi-phase approach keeps each agent's context minimal.

\textbf{Behavioral contracts over API compatibility.} Initial migrations that passed all quality gates still exhibited 19\% behavioral discrepancies in output formatting. For example, the input \texttt{5 3 - 2 -} produced \texttt{\$5 - 3 - 2\$} in Python but \texttt{\$( 5 - 3 ) - 2\$} in early Rust migrations---mathematically equivalent but semantically different. The I/O contract phase eliminated these discrepancies.

\subsection{Metrics}

We measured wall-clock duration, API cost in US dollars, production lines of code, test coverage (line and function/method), and I/O contract match rate (percentage of test cases producing identical output to the source implementation). Each configuration (target language Ã— strategy) was executed three times to characterize run-to-run variance. Results are reported as mean $\pm$ standard deviation where applicable.

\section{Results}

\subsection{Migration Outcomes}

We performed migrations to three structurally different target languages: Rust (a systems language with ownership semantics), Java (a managed language with garbage collection), and Go (a systems language with composition-based design and explicit error handling). We tested both module-by-module and feature-by-feature strategies for each target. All migrations achieved 100\% behavioral equivalence on the 21-case I/O contract. Table~\ref{tab:multilang} summarizes the results.

\begin{table}[h]
\centering
\caption{Migration results by target and strategy (mean, n=3)}
\label{tab:multilang}
\begin{tabular}{@{}llrrrr@{}}
\toprule
Target & Strategy & Dur. (min) & Cost & Msgs & Cov. \\
\midrule
Rust & MbM & 32 & \$8.62 & 918 & 95\% \\
Rust & FbF & 51 & \$8.24 & 856 & 95\% \\
Java & MbM & 45 & \$11.93 & 1177 & 92\% \\
Java & FbF & 47 & \$7.96 & 938 & 80\% \\
Go & MbM & 49 & \$8.42 & 1018 & 71\% \\
Go & FbF & 44 & \$6.32 & 894 & 67\% \\
\bottomrule
\end{tabular}
\end{table}

All 18 migrations completed successfully and passed quality gates. Module-by-module was faster for Rust (32 min vs 51 min) but showed no consistent speed advantage for Java or Go. Feature-by-feature was consistently cheaper for Java (\$7.96 vs \$11.93) and Go (\$6.32 vs \$8.42), while costs were comparable for Rust. Test coverage varied by target language: Rust achieved 95\%, Java 80--92\%, and Go 67--71\%.

With three runs per configuration, we observed substantial run-to-run variance in duration and cost. See Appendix for detailed variance characterization.

\section{Discussion}

\subsection{Why Smaller Contexts Outperform}

The counterintuitive finding that embedding source code degraded performance can be explained by several factors. In transformer-based models, attention complexity scales quadratically with context length~\cite{vaswani2017attention}. Larger contexts increase both latency per token and total tokens processed. When subagents inherit parent context, this cost multiplies across invocations.

Furthermore, large contexts dilute the signal of specific instructions~\cite{liu2024lost}. When source code is embedded alongside instructions, the model must attend to substantially more content to locate relevant information. A focused specification document provides higher signal-to-noise ratio.

Finally, agents exhibit emergent behavior that may not follow explicit instructions~\cite{wei2022emergent}. Despite being told to use embedded content, agents still performed file operations. This suggests that behavioral constraints should be enforced through tool access rather than prompt instructions.

\subsection{Importance of Behavioral Contracts}

Migrations that pass all quality gates may still exhibit behavioral discrepancies. In our experiments, early migrations without I/O contracts showed 19\% of test cases producing different output despite successful compilation, linting, and test passage. Auto-generated tests validate implementation self-consistency but cannot detect semantic drift from the source implementation.

Input-output contracts provide an oracle derived from the source implementation's actual behavior. By capturing exact outputs for representative inputs, contracts enable detection of behavioral differences that might otherwise go unnoticed. This is particularly important for edge cases involving operator precedence, associativity, and error handling where reasonable implementations may differ.

\subsection{Migration Strategy Comparison}

We evaluated two migration strategies on the same subject system: \textit{module-by-module} (vertical slices) and \textit{feature-by-feature} (horizontal slices).

\textbf{Module-by-module} migrates each source module completely before proceeding to the next. The order follows the dependency graph: tokens, AST, error handling, lexer, parser, generator, CLI. Each module is independently testable after migration.

\textbf{Feature-by-feature} migrates horizontal slices across all modules. Each feature (e.g., ``addition operator'') is migrated through lexer, parser, and generator before proceeding to the next feature. This strategy enables incremental I/O validation per feature.

Table~\ref{tab:strategy} presents observed metrics for both strategies across target languages.

\begin{table}[h]
\centering
\caption{Strategy comparison by target language (mean, n=3)}
\label{tab:strategy}
\begin{tabular}{@{}llrrr@{}}
\toprule
Target & Strategy & Cost & Dur. (min) & Msgs \\
\midrule
Rust & MbM & \$8.62 & 32 & 918 \\
Rust & FbF & \$8.24 & 51 & 856 \\
Java & MbM & \$11.93 & 45 & 1177 \\
Java & FbF & \$7.96 & 47 & 938 \\
Go & MbM & \$8.42 & 49 & 1018 \\
Go & FbF & \$6.32 & 44 & 894 \\
\bottomrule
\end{tabular}
\end{table}

Both strategies produced correct output for all 18 runs. Module-by-module was faster for Rust (32 vs 51 min) but showed no speed advantage for Java or Go. Feature-by-feature produced fewer messages and was cheaper for Java (\$7.96 vs \$11.93) and Go (\$6.32 vs \$8.42).

Java module-by-module had the highest cost (\$11.93) and message count (1177), suggesting more error recovery cycles with this combination. With three runs per configuration, we conclude that strategy effects are language-dependent: module-by-module is faster for Rust, while feature-by-feature is cheaper for Java and Go.

\subsection{Tool Usage Patterns}

Table~\ref{tab:tools} shows tool invocation counts across all migrations. These counts reflect how agents interact with the codebase during migration.

\begin{table}[h]
\centering
\caption{Tool invocations by target and strategy (averaged across 3 runs)}
\label{tab:tools}
\begin{tabular}{@{}llrrrr@{}}
\toprule
Target & Strategy & Bash & Read & Write & Edit \\
\midrule
Rust & MbM & 241 & 123 & 29 & 19 \\
Rust & FbF & 185 & 123 & 25 & 49 \\
Java & MbM & 300 & 153 & 46 & 24 \\
Java & FbF & 200 & 142 & 34 & 33 \\
Go & MbM & 300 & 112 & 39 & 12 \\
Go & FbF & 192 & 141 & 29 & 29 \\
\bottomrule
\end{tabular}
\end{table}

Module-by-module strategies used more Bash invocations (241--300) than feature-by-feature (185--200), consistent with more build/test cycles per module. Feature-by-feature migrations showed higher Edit-to-Write ratios (Rust FbF: 49 Edit vs 25 Write; Go FbF: 29 Edit vs 29 Write), indicating more iterative file refinement. Module-by-module favored complete file writes (Go MbM: 12 Edit vs 39 Write), suggesting a write-then-verify approach.

\subsection{Test Coverage}

Table~\ref{tab:coverage} presents test coverage measurements for migrations where coverage tooling was available.

\begin{table}[h]
\centering
\caption{Test coverage by target and strategy (mean, n=3)}
\label{tab:coverage}
\begin{tabular}{@{}llr@{}}
\toprule
Target & Strategy & Line Coverage \\
\midrule
Rust & MbM & 95\% \\
Rust & FbF & 95\% \\
Java & MbM & 92\% \\
Java & FbF & 80\% \\
Go & MbM & 71\% \\
Go & FbF & 67\% \\
\bottomrule
\end{tabular}
\end{table}

Rust migrations achieved the highest coverage (95\%) regardless of strategy, measured using cargo-llvm-cov. Java coverage varied by strategy: module-by-module achieved 92\% while feature-by-feature reached 80\%, measured via JaCoCo. Go achieved the lowest coverage (67--71\%), measured using Go's built-in coverage tooling.

The coverage disparity across languages is consistent across all 18 runs. Rust's high coverage may reflect the language's emphasis on exhaustive pattern matching and the framework's Rust-specific idiom prompts. Coverage variance was lowest for Rust and highest for Go (see Appendix).

\subsection{Code Idiomaticness}

We evaluated each migration for adherence to target language idioms using an LLM-based reviewer. The reviewer examined source files against language-specific criteria: for Rust, proper use of Result/Option, ownership patterns, and pattern matching; for Java, encapsulation, naming conventions, and exception handling; for Go, error handling patterns, interface design, and struct organization. Scores were assigned on a three-tier scale: \textit{Idiomatic} (follows conventions well, would pass code review), \textit{Acceptable} (works but has non-idiomatic patterns), or \textit{Non-idiomatic} (significant style issues).

All six migrations scored \textbf{Idiomatic}. The Rust migrations demonstrated proper error handling with the \texttt{?} operator, correct ownership with \texttt{Box} for recursive structures, and comprehensive pattern matching. Java migrations used sealed interfaces, immutable classes, and proper JavaDoc documentation. Go migrations followed the \texttt{if err != nil} pattern, used idiomatic \texttt{NewXxx} constructors, and maintained clean struct design.

This uniformly high idiomaticness score suggests that the quality gates (Clippy for Rust, Checkstyle for Java, \texttt{go vet} for Go) effectively enforce language conventions during migration. The autonomous feedback loop between code generation and linting appears sufficient to produce code that would pass professional code review.

\subsection{Cost Observations}

Across all 18 migrations, mean costs per configuration ranged from \$6.32 (Go FbF) to \$11.93 (Java MbM). For this 352-line source codebase, this corresponds to approximately \$18 to \$34 per thousand lines of source code.

Feature-by-feature strategies were consistently cheaper than module-by-module for Java (34\% cheaper) and Go (25\% cheaper), while costs were comparable for Rust. The 1.9x ratio between highest and lowest mean costs (\$11.93 vs \$6.32) reflects meaningful differences between configurations rather than run-to-run noise.

\subsection{Scope Characterization and Limitations}

The success of our methodology must be understood within its demonstrated scope. Our subject system represents a deliberately \textit{trivial} case: 352 lines of production code, no external dependencies, low cyclomatic complexity, unidirectional module dependencies, and deterministic input-output behavior. These characteristics define the boundary conditions under which we have validated the approach.

\textbf{What we have proven}: LLM-based multi-agent migration can achieve 100\% behavioral equivalence for small-scale, well-structured codebases across multiple target languages.

\textbf{What remains unproven}: Whether the methodology scales to medium or large codebases with:
\begin{itemize}
\item External library dependencies requiring equivalent library selection in the target language
\item Cross-module circular dependencies or complex initialization ordering
\item Non-deterministic behavior (threading, I/O, randomness)
\item Legacy code patterns lacking clear module boundaries
\item Implicit behavioral contracts not captured by simple I/O testing
\end{itemize}

The 21-case behavioral contract, while comprehensive for our subject system, represents only the most straightforward form of behavioral specification. Systems with stateful behavior, side effects, or complex error recovery may require substantially richer contracts.

\section{Related Work}

This work builds on research in LLM-based code generation, multi-agent systems, and automated software migration.

\textbf{Code generation benchmarks.} Prior work on LLM code generation has focused primarily on single-function or single-file generation. HumanEval~\cite{chen2021codex} evaluates function-level synthesis from docstrings, while AlphaCode~\cite{li2022alphacode} generates solutions to competitive programming problems. More recently, SWE-bench~\cite{jimenez2024swebench} introduced repository-level evaluation using real GitHub issues, representing a shift toward more realistic software engineering tasks. Our work extends this trajectory to complete cross-language migration with behavioral validation.

\textbf{Multi-agent software engineering.} Concurrent with our work, multi-agent LLM systems have been applied to software development. MetaGPT~\cite{hong2024metagpt} encodes software development SOPs into multi-agent workflows with specialized roles (product manager, architect, engineer). ChatDev~\cite{qian2024chatdev} implements waterfall-style development through communicative agents. These systems generate new software from specifications rather than migrating existing codebases, but share architectural principles with our approach: specialized agent roles and iterative refinement through quality gates.

\textbf{LLM-based code translation.} TransAGENT~\cite{yuan2024transagent} presents a multi-agent system for code translation with four specialized agents: translator, syntax fixer, code aligner, and semantic error fixer. The system uses execution alignment to localize errors in translated code, enabling targeted fixes. Our work differs in scope and validation approach: TransAGENT targets function-level translation with execution-based debugging, while we demonstrate end-to-end codebase migration validated against I/O contracts derived from source behavior. The I/O contract approach provides an oracle for behavioral equivalence without requiring execution alignment infrastructure.

\textbf{Industrial-scale migration.} Ziftci et al.~\cite{ziftci2025google} report on LLM-assisted migrations at Google, achieving 74\% LLM-generated code changes across 39 migrations in their monorepo. Their system runs autonomously but addresses same-language migrations (API updates, dependency upgrades) rather than cross-language translation, and relies on existing test suites for validation. Our work complements this by demonstrating cross-language migration where target-language tests do not yet exist, requiring behavioral contracts generated from source execution.

\textbf{Rule-based transformation.} Traditional transpilers rely on rule-based systems such as TXL~\cite{cordy2006txl}, Stratego/XT~\cite{visser2004stratego}, and Rascal~\cite{klint2009rascal}. These require manually specified transformation rules, which become impractical for large API surfaces and cannot adapt to idiomatic target-language patterns. Recent work~\cite{pan2024llmtranslation} shows LLM-based translation outperforms rule-based approaches in accuracy while producing more concise, idiomatic output.

\section{Conclusion}

This paper provides empirical evidence that LLM-based multi-agent systems can automate cross-language code migration for small-scale, well-structured codebases. Through 18 migrations---three runs each for two strategies across three target languages---we validated a four-phase methodology that reliably produces working code across structurally different target languages.

\textbf{Primary finding}: The methodology is language-agnostic and reliable. All 18 migrations achieved 100\% behavioral equivalence. The same four-phase process---I/O contract generation, source analysis, sequential migration, and behavioral review---succeeded for Rust, Java, and Go with only configuration changes.

\textbf{Coverage finding}: Test coverage varied by target language: Rust achieved 95\%, Java ranged from 80--92\%, and Go achieved 67--71\%. Rust's consistency suggests its toolchain and language features support more predictable test generation.

\textbf{Cost finding}: Mean costs ranged from \$6.32 to \$11.93 per migration (1.9x range). Feature-by-feature was consistently cheaper for Java (34\%) and Go (25\%).

\textbf{Strategy finding}: Strategy effects were language-dependent. Module-by-module was faster for Rust (32 vs 51 min) but showed no speed advantage for Java or Go. Feature-by-feature produced fewer messages and lower costs for Java and Go.

Our subject system, \texttt{rpn2tex}, is deliberately trivial: 352 lines of production code, no external dependencies, and deterministic behavior. This establishes a baseline demonstrating that automated migration works at small scale. Future research will determine whether this approach scales to medium-sized codebases (5,000--20,000 LOC) with external dependencies.

\section{Future Work}

The validated methodology and variance characterization from 18 runs provide a foundation for scaling research toward practical legacy modernization. Key questions for future investigation include:

\textbf{Incremental monolith decomposition}: The most promising path to migrating large legacy codebases (PHP, COBOL, aging Java) may not be whole-system translation but incremental extraction. Can agents identify and extract small, well-designed modules suitable for automated migration---analogous to ``extract method'' refactoring at the module level? Each extracted module would meet the characteristics that made our subject system amenable to automation: clear boundaries, low complexity, and minimal dependencies. This approach would enable organizations to progressively decompose monolithic systems into modern microservices, migrating one bounded context at a time while maintaining system integrity.

\textbf{Medium-complexity codebases}: The primary scaling question is whether this methodology extends to systems in the 5,000--20,000 LOC range with external dependencies. Google's experience~\cite{ziftci2025google} suggests LLM-based migration can work at scale for same-language transformations; cross-language migration at scale remains unproven. At larger scale, cost and duration variance become more critical for practical deployment decisions.

\textbf{Coverage improvement}: The 28-percentage-point coverage gap between Rust (95\%) and Go (67--71\%) persisted across all runs. Investigating prompt modifications to improve Go and Java coverage is a priority for production readiness.

\textbf{Dependency management}: Can agents automatically select equivalent libraries in the target ecosystem, or does this require human guidance? A PHP application using Laravel would need equivalent frameworks in the target language---this library mapping problem is a prerequisite for medium-scale migration.

\textbf{Stateful systems}: How can behavioral contracts capture systems with internal state, side effects, or non-deterministic behavior? Our current I/O contract approach assumes pure, deterministic functions. Real legacy systems often have database interactions, session state, and external API calls that require richer behavioral specifications.

\textbf{Strategy selection guidelines}: Strategy effects were language-dependent: module-by-module was faster for Rust but feature-by-feature was cheaper for Java and Go. Developing heuristics for strategy selection based on target language and codebase characteristics would improve migration efficiency.

This baseline establishes that automated migration works reliably at small scale with 100\% behavioral equivalence across 18 runs. The challenge now is extending these techniques to enable practical legacy modernization---helping organizations escape the maintenance burden of aging codebases by providing a reliable, cost-effective path to modern languages and architectures.

\section*{Acknowledgments}

This research was conducted using the Claude Agent SDK with Claude 3.5 Sonnet and Claude 3.5 Haiku models. The experimental framework and analysis were developed collaboratively between human and AI researchers.

\section*{Data Availability}

The migration framework, experimental data, and all generated code are available at: \url{https://github.com/jmf-pobox/llm-migration-research}

\onecolumn
\appendix

\section{Complete Migration Metrics}

Table~\ref{tab:complete} presents averaged metrics across all 18 migrations (3 runs per configuration).

\begin{table}[h]
\centering
\caption{Complete migration metrics for all configurations (mean of 3 runs)}
\label{tab:complete}
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
& \multicolumn{3}{c}{Module-by-Module} & \multicolumn{3}{c}{Feature-by-Feature} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Metric & Rust & Java & Go & Rust & Java & Go \\
\midrule
\multicolumn{7}{@{}l}{\textit{Lines of Code}} \\
Production LOC & 617 & 542 & 575 & 462 & 404 & 405 \\
Test LOC & 1,665 & 2,265 & 2,608 & 1,362 & 948 & 1,320 \\
Test/Prod ratio & 2.7x & 4.2x & 4.5x & 2.9x & 2.3x & 3.3x \\
\midrule
\multicolumn{7}{@{}l}{\textit{Timing (mean $\pm$ std)}} \\
Wall clock (min) & $32 \pm 6$ & $45 \pm 15$ & $49 \pm 18$ & $51 \pm 8$ & $47 \pm 10$ & $44 \pm 12$ \\
\midrule
\multicolumn{7}{@{}l}{\textit{Cost (mean $\pm$ std)}} \\
Total cost (USD) & $8.62 \pm 0.64$ & $11.93 \pm 1.93$ & $8.42 \pm 0.50$ & $8.24 \pm 1.18$ & $7.96 \pm 0.31$ & $6.32 \pm 0.11$ \\
\midrule
\multicolumn{7}{@{}l}{\textit{Agent Activity (mean)}} \\
Total messages & 918 & 1,177 & 1,018 & 856 & 938 & 894 \\
\midrule
\multicolumn{7}{@{}l}{\textit{Tool Invocations (mean)}} \\
Bash & 241 & 300 & 300 & 185 & 200 & 192 \\
Read & 123 & 153 & 112 & 123 & 142 & 141 \\
Write & 29 & 46 & 39 & 25 & 34 & 29 \\
Edit & 19 & 24 & 12 & 49 & 33 & 29 \\
\midrule
\multicolumn{7}{@{}l}{\textit{Quality (mean $\pm$ std)}} \\
Line coverage (\%) & $95.0 \pm 2.3$ & $91.7 \pm 4.3$ & $71.1 \pm 7.5$ & $94.9 \pm 0.2$ & $80.3 \pm 6.6$ & $66.7 \pm 2.3$ \\
I/O match rate & 100\% & 100\% & 100\% & 100\% & 100\% & 100\% \\
Idiomaticness & Idiomatic & Idiomatic & Idiomatic & Idiomatic & Idiomatic & Idiomatic \\
\bottomrule
\end{tabular}
\end{table}

\vspace{1em}
\noindent\textbf{Notes:}
\begin{itemize}[nosep]
\item All migrations used Claude 3.5 Sonnet for code generation and Claude 3.5 Haiku for analysis/review
\item Source codebase: 352 lines of Python production code + 204 lines of test code (0.6x test/prod ratio)
\item Agents generated 2--5x more test code than production code, a 4--7x increase over source ratio
\item Each configuration was run 3 times; values represent means with standard deviations where applicable
\item All 18 runs achieved 100\% behavioral equivalence on the 21-case I/O contract
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
